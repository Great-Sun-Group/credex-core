<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Connectors Workflow and Infrastructure</title>
    <link rel="stylesheet" href="../../style.css" />
  </head>
  <body>
    <div class="container">
      <h1>Connectors Workflow and Infrastructure</h1>

      <p>
        This document provides an overview of the connectors workflow and the
        associated infrastructure managed by Terraform.
      </p>

      <h2>Connectors Workflow</h2>

      <p>
        The <code>.github/workflows/connectors.yml</code> file defines a GitHub
        Actions workflow for deploying and managing AWS infrastructure
        components. Key aspects of this workflow include:
      </p>

      <ol>
        <li>
          <strong>Trigger</strong>: The workflow is manually triggered
          (<code>workflow_dispatch</code>).
        </li>
        <li>
          <strong>Environment</strong>: The workflow determines the environment
          (development, staging, or production) based on the Git branch.
        </li>
        <li>
          <strong>AWS Region</strong>: The workflow is configured to use the
          af-south-1 (Cape Town) region.
        </li>
        <li>
          <strong>Steps</strong>:
          <ul>
            <li>Checkout code</li>
            <li>Configure AWS credentials</li>
            <li>Setup Terraform</li>
            <li>
              Create S3 bucket and DynamoDB table for Terraform state (if they
              don't exist)
            </li>
            <li>Initialize Terraform</li>
            <li>Plan Terraform changes</li>
            <li>Apply Terraform changes</li>
            <li>Retrieve and output infrastructure details</li>
          </ul>
        </li>
      </ol>

      <h2>Terraform Configuration</h2>

      <p>
        The Terraform configuration for the connectors is organized into a main
        module and a shared resources submodule:
      </p>

      <h3>1. Main Connectors Module</h3>
      <p>
        The entry point is <code>terraform/modules/connectors/main.tf</code>.
        This file:
      </p>
      <ul>
        <li>
          Defines local variables for common tags and the full domain name.
        </li>
        <li>
          Generates an RSA key pair using the
          <code>tls_private_key</code> resource.
        </li>
        <li>
          Calls the Shared Resources submodule, passing necessary variables and
          the generated public key.
        </li>
      </ul>

      <h3>2. Shared Resources Submodule</h3>
      <p>
        Located at <code>terraform/modules/connectors/shared_resources/</code>,
        this submodule is responsible for creating all the actual AWS resources.
      </p>

      <h2>Infrastructure Components</h2>

      <p>
        The connectors workflow and Terraform configuration manage the following
        key infrastructure components:
      </p>

      <ol>
        <li>
          <strong>Networking</strong>: VPC, subnets, Internet Gateway, NAT
          Gateway, and route tables.
        </li>
        <li>
          <strong>Security</strong>: Security groups for ALB, ECS tasks, and
          Neo4j.
        </li>
        <li>
          <strong>Load Balancing</strong>: Application Load Balancer with HTTPS
          listener and target group.
        </li>
        <li><strong>Certificates</strong>: ACM certificate for HTTPS.</li>
        <li><strong>Access</strong>: EC2 key pair for SSH access.</li>
        <li>
          <strong>Container Services</strong>: ECR for storing Docker images and
          ECS for running containers.
        </li>
        <li>
          <strong>Monitoring</strong>: CloudWatch Logs for log management.
        </li>
        <li>
          <strong>Identity and Access Management</strong>: IAM roles for various
          services.
        </li>
      </ol>

      <h2>EC2 Instances and Application Deployment</h2>

      <p>
        The connectors workflow sets up the infrastructure that supports EC2
        instances for Neo4j databases and the application deployment:
      </p>
      <ul>
        <li>The VPC and subnets provide the network environment.</li>
        <li>Security groups control access to the instances.</li>
        <li>The key pair allows SSH access to the instances.</li>
      </ul>

      <p>EC2 instances and application deployment are managed as follows:</p>
      <ul>
        <li>
          <strong>Neo4j Databases</strong>: The databases module creates EC2
          instances for Neo4j databases.
        </li>
        <li>
          <strong>Application Deployment</strong>: The application is deployed
          using AWS Fargate, which is a serverless compute engine for containers
          that works with Amazon ECS.
        </li>
      </ul>

      <h3>Fargate Deployment</h3>

      <p>
        For the Fargate deployment, the following parts of the infrastructure
        are used:
      </p>

      <ol>
        <li>
          <strong>VPC and Subnets</strong>: Fargate tasks run in the private
          subnets of the VPC.
        </li>
        <li>
          <strong>Security Groups</strong>: The ECS tasks security group
          controls access to the Fargate tasks.
        </li>
        <li>
          <strong>Load Balancer</strong>: The Application Load Balancer routes
          traffic to the Fargate tasks.
        </li>
        <li>
          <strong>Target Group</strong>: The ALB target group is used to
          register the Fargate tasks.
        </li>
        <li>
          <strong>ECS Cluster</strong>: An ECS cluster is used to manage the
          Fargate tasks.
        </li>
        <li>
          <strong>ECR</strong>: Stores the Docker images used by the Fargate
          tasks.
        </li>
      </ol>

      <p>
        The actual creation and management of the ECS cluster, task definitions,
        and services for Fargate are handled in the
        <code>app.yml</code> workflow and associated terraform module.
      </p>

      <h2>Infrastructure Outputs</h2>

      <p>
        After applying the Terraform changes, the workflow outputs key
        infrastructure details, including:
      </p>

      <ul>
        <li>VPC ID</li>
        <li>Subnet IDs</li>
        <li>Neo4j Security Group ID</li>
        <li>Key Pair Name</li>
        <li>ALB Security Group ID</li>
      </ul>

      <p>
        These outputs are crucial for other modules and workflows that depend on
        the infrastructure created by the connectors workflow. These values are
        printed to the workflow logs, and are made accessible programmatically
        to the other modules.
      </p>

      <h2>Conclusion</h2>

      <p>
        The connectors workflow and associated Terraform configuration provide a
        flexible and modular approach to managing AWS infrastructure. It sets up
        the core networking, security, and load balancing components that
        support both EC2-based Neo4j databases and Fargate-based application
        deployment. The workflow ensures that the necessary infrastructure is in
        place before the databases and application are deployed.
      </p>
    </div>
  </body>
</html>
